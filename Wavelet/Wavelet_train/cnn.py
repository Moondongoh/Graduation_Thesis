import os
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, OneHotEncoder

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader

# import librosa # WAV íŒŒì¼ ë¡œë“œì—ëŠ” ì‚¬ìš©ë˜ì§€ ì•Šì§€ë§Œ, ë”ë¯¸ WAV ìƒì„± ì‹œ í•„ìš”í•  ìˆ˜ ìˆìŒ
# import pywt # ì›¨ì´ë¸”ë¦¿ ë³€í™˜ì—ëŠ” ì‚¬ìš©ë˜ì§€ ì•Šì§€ë§Œ, ë”ë¯¸ WAV ìƒì„± ì‹œ ì‚¬ìš©ë  ìˆ˜ ìˆìŒ
import soundfile as sf  # ë”ë¯¸ íŒŒì¼ ìƒì„±ì„ ìœ„í•¨

# GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
# CUDA (NVIDIA GPU) ì‚¬ìš© ê°€ëŠ¥í•˜ë©´ 'cuda', ì•„ë‹ˆë©´ 'cpu'
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")


# --- ìˆ˜ì •ëœ load_labels_and_data_paths í•¨ìˆ˜ ---
def load_labels_and_data_paths(wavelet_data_path, label_file_path):
    """
    ë¼ë²¨ íŒŒì¼ê³¼ ì›¨ì´ë¸”ë¦¿ ë³€í™˜ëœ TXT ë°ì´í„° ê²½ë¡œë¥¼ ë¡œë“œí•˜ê³  ë§¤ì¹­í•©ë‹ˆë‹¤.

    Args:
        wavelet_data_path (str): ì›¨ì´ë¸”ë¦¿ ë³€í™˜ëœ TXT íŒŒì¼ë“¤ì´ ìˆëŠ” ë””ë ‰í† ë¦¬ ê²½ë¡œ (ì˜ˆ: 'Wavelet_Transformed_Data').
        label_file_path (str): ë¼ë²¨ ì •ë³´ê°€ ë‹´ê¸´ CSV ë˜ëŠ” TXT íŒŒì¼ ê²½ë¡œ (ì˜ˆ: 'REFERENCE.csv').

    Returns:
        tuple: (list of wavelet TXT file paths, list of corresponding labels)
               ë˜ëŠ” (None, None) ì˜¤ë¥˜ ë°œìƒ ì‹œ.
    """
    data_file_paths = []
    labels = []
    skipped_files_count = 0

    if not os.path.exists(label_file_path):
        print(
            f"ì˜¤ë¥˜: ë¼ë²¨ íŒŒì¼ '{label_file_path}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”."
        )
        return None, None

    try:
        df_labels = pd.read_csv(
            label_file_path, header=None, names=["filename_prefix", "label"]
        )
        print(f"'{label_file_path}'ì—ì„œ {len(df_labels)}ê°œì˜ ë¼ë²¨ ë¡œë“œ ì™„ë£Œ.")

        for index, row in df_labels.iterrows():
            # íŒŒì¼ëª… ì ‘ë‘ì‚¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì›¨ì´ë¸”ë¦¿ ë³€í™˜ëœ TXT íŒŒì¼ëª… êµ¬ì„±
            # ì˜ˆ: 'a0001' -> 'a0001_wavelet.txt'
            txt_filename = f"{row['filename_prefix']}_wavelet.txt"
            txt_file_path = os.path.join(wavelet_data_path, txt_filename)

            if os.path.exists(txt_file_path):
                data_file_paths.append(txt_file_path)
                labels.append(row["label"])
            else:
                # print(f"ê²½ê³ : ì›¨ì´ë¸”ë¦¿ TXT íŒŒì¼ '{txt_file_path}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê±´ë„ˆëœœ.")
                skipped_files_count += 1

    except pd.errors.EmptyDataError:
        print(f"ì˜¤ë¥˜: ë¼ë²¨ íŒŒì¼ '{label_file_path}'ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
        return None, None
    except Exception as e:
        print(f"ë¼ë²¨ íŒŒì¼ ë¡œë“œ ë˜ëŠ” ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None, None

    if skipped_files_count > 0:
        print(
            f"ì´ {skipped_files_count}ê°œì˜ ì›¨ì´ë¸”ë¦¿ TXT íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ê±´ë„ˆë›°ì—ˆìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ ë‹¤ì‹œ í™•ì¸í•´ì£¼ì„¸ìš”."
        )
    return data_file_paths, labels


# --- ì´ì „ ì½”ë“œì™€ ë™ì¼í•œ preprocess_labels í•¨ìˆ˜ ---
def preprocess_labels(labels):
    """
    ë¼ë²¨ (ë¬¸ìì—´ ë˜ëŠ” ìˆ«ì)ì„ ìˆ˜ì¹˜í˜• (ì¸ì½”ë”©) ë° ì›-í•« ì¸ì½”ë”© í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    "1ì´ ì •ìƒ, 0ì´ ë¹„ì •ìƒ" ê·œì¹™ì„ ë”°ë¦…ë‹ˆë‹¤.
    """
    label_encoder = LabelEncoder()
    encoded_labels = label_encoder.fit_transform(labels)

    mapped_class_names = [
        f"Class {cls_val} ({'ë¹„ì •ìƒ' if cls_val == 0 else 'ì •ìƒ'})"
        for cls_val in label_encoder.classes_
    ]

    print(f"ì›ë³¸ ë¼ë²¨ ì¢…ë¥˜: {np.unique(labels)}")
    print(f"ë‚´ë¶€ ì¸ì½”ë”©ëœ ë¼ë²¨ ì¢…ë¥˜: {np.unique(encoded_labels)}")
    print(f"ë§¤í•‘ëœ í´ë˜ìŠ¤ ì´ë¦„ (0:ë¹„ì •ìƒ, 1:ì •ìƒ): {mapped_class_names}")

    onehot_encoder = OneHotEncoder(sparse_output=False)
    onehot_labels = onehot_encoder.fit_transform(encoded_labels.reshape(-1, 1))
    print(f"ì›-í•« ì¸ì½”ë”©ëœ ë¼ë²¨ í˜•íƒœ: {onehot_labels.shape}")

    return encoded_labels, onehot_labels, label_encoder, mapped_class_names


# --- PyTorch Custom Dataset ì •ì˜ (ì›¨ì´ë¸”ë¦¿ ë°ì´í„°ìš©) ---
class WaveletTextDataset(Dataset):
    def __init__(self, data_file_paths, labels, max_sequence_length):
        self.data_file_paths = data_file_paths
        # ë¼ë²¨ì€ CrossEntropyLossë¥¼ ìœ„í•´ long íƒ€ì…ìœ¼ë¡œ ì €ì¥
        self.labels = torch.tensor(labels, dtype=torch.long)
        self.max_sequence_length = max_sequence_length
        self.cached_data = {}  # ë°ì´í„°ë¥¼ ë¯¸ë¦¬ ë¡œë“œí•˜ì—¬ ìºì‹±

        print(f"ë°ì´í„°ì…‹ ì´ˆê¸°í™” ì¤‘. ì´ {len(data_file_paths)}ê°œ íŒŒì¼ ì²˜ë¦¬ ì˜ˆìƒ...")
        for i, path in enumerate(self.data_file_paths):
            try:
                y_data = np.loadtxt(path, dtype=np.float32)

                if len(y_data) < self.max_sequence_length:
                    pad_width = self.max_sequence_length - len(y_data)
                    y_padded = np.pad(
                        y_data, (0, pad_width), "constant", constant_values=0
                    )
                else:
                    y_padded = y_data[: self.max_sequence_length]

                # PyTorch Conv1DëŠ” (batch_size, channels, sequence_length) í˜•íƒœë¥¼ ê¸°ëŒ€
                # ì—¬ê¸°ì„œëŠ” channels=1 ì´ë¯€ë¡œ (1, sequence_length) í˜•íƒœë¡œ ì €ì¥
                self.cached_data[i] = torch.tensor(
                    y_padded.reshape(1, -1), dtype=torch.float32
                )

            except Exception as e:
                print(
                    f"  ë°ì´í„°ì…‹ ë¡œë“œ ì˜¤ë¥˜: '{os.path.basename(path)}' - {e}. í•´ë‹¹ ì¸ë±ìŠ¤ ê±´ë„ˆëœœ."
                )
                self.cached_data[i] = None  # ì˜¤ë¥˜ ë°œìƒ íŒŒì¼ì€ Noneìœ¼ë¡œ í‘œì‹œ

        # ì˜¤ë¥˜ ì—†ì´ ë¡œë“œëœ ë°ì´í„°ë§Œ ì‚¬ìš©í•˜ë„ë¡ í•„í„°ë§
        self.valid_indices = [
            i for i, data in self.cached_data.items() if data is not None
        ]
        print(f"ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ. ìœ íš¨í•œ íŒŒì¼ ìˆ˜: {len(self.valid_indices)}")

    def __len__(self):
        return len(self.valid_indices)

    def __getitem__(self, idx):
        original_idx = self.valid_indices[idx]
        data = self.cached_data[original_idx]
        label = self.labels[original_idx]
        return data, label


# --- PyTorch 1D CNN ëª¨ë¸ ì •ì˜ ---
class OneDCNN(nn.Module):
    def __init__(self, num_classes, input_length):  # input_lengthë¥¼ íŒŒë¼ë¯¸í„°ë¡œ ë°›ìŒ
        super(OneDCNN, self).__init__()
        self.conv_layers = nn.Sequential(
            # Conv1dëŠ” (in_channels, out_channels, kernel_size)
            nn.Conv1d(in_channels=1, out_channels=32, kernel_size=3),
            nn.ReLU(),
            nn.MaxPool1d(kernel_size=2),
            nn.Dropout(0.3),
            nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3),
            nn.ReLU(),
            nn.MaxPool1d(kernel_size=2),
            nn.Dropout(0.3),
        )

        # Flattening ì „ feature map í¬ê¸° ê³„ì‚°
        self._calculate_flatten_size(input_length)

        self.fc_layers = nn.Sequential(
            nn.Linear(self._flatten_size, 100),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(100, num_classes),
            # CrossEntropyLossëŠ” ë‚´ë¶€ì ìœ¼ë¡œ Softmaxë¥¼ í¬í•¨í•˜ë¯€ë¡œ ìµœì¢… SoftmaxëŠ” í•„ìš” ì—†ìŒ
        )

    def _calculate_flatten_size(self, input_length):
        # ë”ë¯¸ í…ì„œë¥¼ ë§Œë“¤ì–´ì„œ Conv ì¸µì„ í†µê³¼ì‹œì¼œ ì¶œë ¥ í¬ê¸°ë¥¼ ê³„ì‚°
        with torch.no_grad():
            dummy_input = torch.zeros(1, 1, input_length)  # (batch, channels, length)
            dummy_output = self.conv_layers(dummy_input)
            self._flatten_size = (
                dummy_output.numel() // dummy_output.shape[0]
            )  # ë°°ì¹˜ ì°¨ì› ì œì™¸

    def forward(self, x):
        x = self.conv_layers(x)
        x = x.view(x.size(0), -1)  # Flatten
        x = self.fc_layers(x)
        return x


# --- ì‚¬ìš© ì˜ˆì‹œ (ë©”ì¸ ì½”ë“œ) ---
if __name__ == "__main__":
    # ğŸš¨ğŸš¨ğŸš¨ ì‹¤ì œ ë°ì´í„°ì…‹ ê²½ë¡œì™€ REFERENCE.csv ê²½ë¡œë¡œ ë³€ê²½í•´ì•¼ í•©ë‹ˆë‹¤ ğŸš¨ğŸš¨ğŸš¨
    # ==========================================================ê²½ë¡œ ì§€ì •==========================================================
    wavelet_transformed_data_folder = (
        r"D:\MDO\heartbeat\Wavelet/Wavelet_Transformed_Data"
    )
    label_file_path = r"D:\MDO\heartbeat\Wavelet/Wavelet_Transformed_Data/REFERENCE.csv"

    # â—â—â— í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ ë”ë¯¸ íŒŒì¼ ë° í´ë” ìƒì„± (ì‹¤ì œ ì‚¬ìš© ì‹œì—ëŠ” ì´ ë¶€ë¶„ ì‚­ì œ) â—â—â—
    if not os.path.exists(wavelet_transformed_data_folder) or not os.path.exists(
        label_file_path
    ):
        print("\n--- í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ë”ë¯¸ ì›¨ì´ë¸”ë¦¿ ë³€í™˜ ë°ì´í„° í´ë” ë° íŒŒì¼ ìƒì„± ---")
        os.makedirs(wavelet_transformed_data_folder, exist_ok=True)
        print(f"ë”ë¯¸ í´ë” '{wavelet_transformed_data_folder}' ìƒì„± ì™„ë£Œ.")

        dummy_labels_data = [
            ("a0001", 1),  # ì •ìƒ
            ("a0002", 0),  # ë¹„ì •ìƒ
            ("b0001", 1),  # ì •ìƒ
            ("c0001", 1),  # ì •ìƒ
            ("c0002", 0),  # ë¹„ì •ìƒ
            ("a0003", 1),
            ("b0002", 0),
            ("c0003", 1),
            ("a0004", 0),
            ("b0003", 1),
            ("d0001", 1),
            ("d0002", 0),
            ("e0001", 1),
            ("f0001", 0),  # ì¶”ê°€ ë”ë¯¸ ë°ì´í„°
        ]
        dummy_df = pd.DataFrame(dummy_labels_data, columns=["filename_prefix", "label"])
        dummy_df.to_csv(label_file_path, index=False, header=False)
        print(f"ë”ë¯¸ '{label_file_path}' ìƒì„± ì™„ë£Œ (ë¼ë²¨: 0=ë¹„ì •ìƒ, 1=ì •ìƒ).")

        # ë”ë¯¸ ì›¨ì´ë¸”ë¦¿ ë³€í™˜ TXT íŒŒì¼ ìƒì„±
        # ì‹¤ì œ ì›¨ì´ë¸”ë¦¿ ë³€í™˜ì„ ê±°ì¹œ ë°ì´í„°ëŠ” ê¸¸ì´ê°€ ê¸¸ ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ì ì ˆí•œ ë”ë¯¸ ê¸¸ì´ ì„¤ì •
        for prefix, _ in dummy_labels_data:
            dummy_txt_path = os.path.join(
                wavelet_transformed_data_folder, f"{prefix}_wavelet.txt"
            )
            # ë”ë¯¸ ë°ì´í„° ê¸¸ì´ë¥¼ ë¬´ì‘ìœ„ë¡œ í•˜ì—¬ íŒ¨ë”©/ìë¥´ê¸° í…ŒìŠ¤íŠ¸ (ì‹¤ì œ ì›¨ì´ë¸”ë¦¿ ë°ì´í„°ëŠ” ë‹¤ë¥¼ ìˆ˜ ìˆìŒ)
            dummy_data_length = np.random.randint(
                1000, 5000
            )  # ì˜ˆë¥¼ ë“¤ì–´, 1000~5000ê°œì˜ ê³„ìˆ˜
            dummy_data = np.random.uniform(-1, 1, dummy_data_length).astype(np.float32)
            np.savetxt(dummy_txt_path, dummy_data, fmt="%f", delimiter="\n")
        print("--- ë”ë¯¸ ì›¨ì´ë¸”ë¦¿ TXT íŒŒì¼ ìƒì„± ì™„ë£Œ. ì´ì œ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ---")
    # â—â—â— ë”ë¯¸ íŒŒì¼ ìƒì„± ë â—â—â—

    # 1. ë¼ë²¨ ë° ì›¨ì´ë¸”ë¦¿ ë³€í™˜ëœ ë°ì´í„° ê²½ë¡œ ë¡œë“œ
    data_file_paths, raw_labels = load_labels_and_data_paths(
        wavelet_transformed_data_folder, label_file_path
    )

    if data_file_paths and raw_labels:
        print("\n--- ë¼ë²¨ ì „ì²˜ë¦¬ ---")
        # encoded_labels (ì •ìˆ˜í˜•)ì™€ onehot_labels (ì›-í•«) ëª¨ë‘ ì–»ìŠµë‹ˆë‹¤.
        encoded_labels, onehot_labels, label_encoder_obj, class_names_mapping = (
            preprocess_labels(raw_labels)
        )

        print(f"\në¡œë“œëœ ìœ íš¨ ì›¨ì´ë¸”ë¦¿ TXT íŒŒì¼ ìˆ˜: {len(data_file_paths)}")
        print(f"ë¡œë“œëœ ìœ íš¨ ë¼ë²¨ ìˆ˜: {len(onehot_labels)}")

        # 2. í›ˆë ¨ ì„¸íŠ¸ì™€ ê²€ì¦ ì„¸íŠ¸ë¡œ ë¶„ë¦¬
        # PyTorch Datasetì— ì „ë‹¬í•˜ê¸° ìœ„í•´ ì •ìˆ˜í˜• ë¼ë²¨(encoded_labels)ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
        X_train_paths, X_val_paths, y_train_encoded, y_val_encoded = train_test_split(
            data_file_paths,
            encoded_labels,
            test_size=0.2,
            random_state=42,
            stratify=encoded_labels,
        )

        print(f"\ní›ˆë ¨ ì„¸íŠ¸ í¬ê¸° (ê²½ë¡œ): {len(X_train_paths)}")
        print(f"ê²€ì¦ ì„¸íŠ¸ í¬ê¸° (ê²½ë¡œ): {len(X_val_paths)}")

        # 3. PyTorch Dataset ë° DataLoader ì¤€ë¹„
        # ğŸš¨ğŸš¨ğŸš¨ MAX_SEQUENCE_LENGTH_WAVELETì€ ì‹¤ì œ ì›¨ì´ë¸”ë¦¿ ë³€í™˜ ë°ì´í„°ì˜ ê¸¸ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤.
        # ëª¨ë“  ì›¨ì´ë¸”ë¦¿ ê³„ìˆ˜ì˜ ê¸¸ì´ë¥¼ ë¶„ì„í•˜ì—¬ ì ì ˆí•œ ê°’ì„ ì„¤ì •í•˜ì„¸ìš”.
        # ì˜ˆë¥¼ ë“¤ì–´, ì›¨ì´ë¸”ë¦¿ ë³€í™˜ëœ ë°ì´í„°ì˜ í‰ê·  ê¸¸ì´ ë˜ëŠ” ìµœëŒ€ ê¸¸ì´ë¥¼ ê³ ë ¤í•©ë‹ˆë‹¤.
        MAX_SEQUENCE_LENGTH_WAVELET = (
            2000  # ì˜ˆì‹œ: ì›¨ì´ë¸”ë¦¿ ê³„ìˆ˜ê°€ ì´ ì •ë„ ê¸¸ì´ë¥¼ ê°€ì§ˆ ìˆ˜ ìˆë‹¤ê³  ê°€ì •
        )

        print(f"\n--- í›ˆë ¨ ë°ì´í„°ì…‹ ìƒì„± (ì›¨ì´ë¸”ë¦¿ ë°ì´í„° ì‚¬ìš©) ---")
        # WaveletTextDatasetì— ì •ìˆ˜í˜• ë¼ë²¨(y_train_encoded)ì„ ì „ë‹¬
        train_dataset = WaveletTextDataset(
            X_train_paths, y_train_encoded, MAX_SEQUENCE_LENGTH_WAVELET
        )
        train_loader = DataLoader(
            train_dataset, batch_size=32, shuffle=True, num_workers=0
        )  # num_workersëŠ” í™˜ê²½ì— ë”°ë¼ ì¡°ì • (Windowsì—ì„œëŠ” 0 ê¶Œì¥)
        print(f"í›ˆë ¨ ë°ì´í„°ì…‹ í¬ê¸°: {len(train_dataset)}")

        print(f"\n--- ê²€ì¦ ë°ì´í„°ì…‹ ìƒì„± (ì›¨ì´ë¸”ë¦¿ ë°ì´í„° ì‚¬ìš©) ---")
        # WaveletTextDatasetì— ì •ìˆ˜í˜• ë¼ë²¨(y_val_encoded)ì„ ì „ë‹¬
        val_dataset = WaveletTextDataset(
            X_val_paths, y_val_encoded, MAX_SEQUENCE_LENGTH_WAVELET
        )
        val_loader = DataLoader(
            val_dataset, batch_size=32, shuffle=False, num_workers=0
        )
        print(f"ê²€ì¦ ë°ì´í„°ì…‹ í¬ê¸°: {len(val_dataset)}")

        # 4. PyTorch 1D CNN ëª¨ë¸ êµ¬ì¶• ë° í•™ìŠµ
        num_classes = onehot_labels.shape[
            1
        ]  # num_classesëŠ” ì›-í•« ì¸ì½”ë”©ëœ ë¼ë²¨ì˜ ì»¬ëŸ¼ ìˆ˜
        # ëª¨ë¸ ìƒì„± ì‹œ MAX_SEQUENCE_LENGTH_WAVELETì„ ì „ë‹¬í•˜ì—¬ Flatten í¬ê¸° ê³„ì‚°ì— ì‚¬ìš©
        model = OneDCNN(
            num_classes=num_classes, input_length=MAX_SEQUENCE_LENGTH_WAVELET
        ).to(device)

        # ì˜µí‹°ë§ˆì´ì €ì™€ ì†ì‹¤ í•¨ìˆ˜ ì •ì˜
        optimizer = optim.Adam(model.parameters(), lr=0.001)
        # CrossEntropyLossëŠ” íƒ€ê²Ÿì´ í´ë˜ìŠ¤ ì¸ë±ìŠ¤ (ì •ìˆ˜, Long íƒ€ì…)ì—¬ì•¼ í•˜ë©°, ë‚´ë¶€ì ìœ¼ë¡œ Softmax í¬í•¨
        criterion = nn.CrossEntropyLoss()

        print("\n--- PyTorch 1D CNN ëª¨ë¸ í•™ìŠµ ì‹œì‘ (ì›¨ì´ë¸”ë¦¿ ë³€í™˜ ë°ì´í„°) ---")
        num_epochs = 10  # ì—í¬í¬ ìˆ˜ ì¡°ì •

        for epoch in range(num_epochs):
            model.train()  # í›ˆë ¨ ëª¨ë“œ
            running_loss = 0.0
            correct_train = 0
            total_train = 0

            for i, (inputs, labels) in enumerate(train_loader):
                inputs, labels = inputs.to(device), labels.to(device)

                optimizer.zero_grad()  # ê¸°ìš¸ê¸° ì´ˆê¸°í™”
                outputs = model(inputs)
                loss = criterion(outputs, labels)  # outputsëŠ” float, labelsëŠ” long
                loss.backward()  # ì—­ì „íŒŒ
                optimizer.step()  # ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸

                running_loss += loss.item() * inputs.size(0)
                _, predicted = torch.max(
                    outputs.data, 1
                )  # ì˜ˆì¸¡ì€ outputsì—ì„œ ê°€ì¥ ë†’ì€ í™•ë¥ ì˜ ì¸ë±ìŠ¤
                total_train += labels.size(0)
                correct_train += (predicted == labels).sum().item()

            epoch_loss = running_loss / len(train_dataset)
            epoch_acc = 100 * correct_train / total_train

            # ê²€ì¦ ë‹¨ê³„
            model.eval()  # í‰ê°€ ëª¨ë“œ
            val_loss = 0.0
            correct_val = 0
            total_val = 0
            with torch.no_grad():  # ê¸°ìš¸ê¸° ê³„ì‚° ë¹„í™œì„±í™”
                for inputs_val, labels_val in val_loader:
                    inputs_val, labels_val = inputs_val.to(device), labels_val.to(
                        device
                    )
                    outputs_val = model(inputs_val)
                    loss_val = criterion(outputs_val, labels_val)

                    val_loss += loss_val.item() * inputs_val.size(0)
                    _, predicted_val = torch.max(outputs_val.data, 1)
                    total_val += labels_val.size(0)
                    correct_val += (predicted_val == labels_val).sum().item()

            val_epoch_loss = val_loss / len(val_dataset)
            val_epoch_acc = 100 * correct_val / total_val

            print(
                f"Epoch [{epoch+1}/{num_epochs}], "
                f"Train Loss: {epoch_loss:.4f}, Train Acc: {epoch_acc:.2f}%, "
                f"Val Loss: {val_epoch_loss:.4f}, Val Acc: {val_epoch_acc:.2f}%"
            )

        print("PyTorch 1D CNN ëª¨ë¸ í•™ìŠµ ì™„ë£Œ (ì›¨ì´ë¸”ë¦¿ ë³€í™˜ ë°ì´í„°).")

        # --- ëª¨ë¸ ì €ì¥ ---
        # ==========================================================ê²½ë¡œ ì§€ì •==========================================================
        model_save_dir = "./Wavelet/model"  # ì €ì¥ ê²½ë¡œ ìˆ˜ì •
        os.makedirs(model_save_dir, exist_ok=True)
        model_save_path_wavelet = os.path.join(model_save_dir, "wavelet_cnn_model.pth")

        # ëª¨ë¸ì˜ ìƒíƒœ ì‚¬ì „(state_dict) ì €ì¥
        torch.save(model.state_dict(), model_save_path_wavelet)
        print(
            f"\nëª¨ë¸ì´ ë‹¤ìŒ ê²½ë¡œì— ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: '{os.path.abspath(model_save_path_wavelet)}'"
        )

        # --- ì €ì¥ëœ ëª¨ë¸ì„ ë‹¤ì‹œ ë¡œë“œí•˜ëŠ” ì˜ˆì‹œ (í™•ì¸ìš©) ---
        # loaded_model = OneDCNN(num_classes=num_classes, input_length=MAX_SEQUENCE_LENGTH_WAVELET).to(device)
        # loaded_model.load_state_dict(torch.load(model_save_path_wavelet))
        # loaded_model.eval() # í‰ê°€ ëª¨ë“œë¡œ ì „í™˜ (Dropout ë“±ì´ ë¹„í™œì„±í™”ë¨)
        # print(f"\nëª¨ë¸ì´ '{model_save_path_wavelet}'ì—ì„œ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
        # # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ (ì˜ˆ: í•œ ë°°ì¹˜ ê°€ì ¸ì™€ì„œ ì¶”ë¡ )
        # # with torch.no_grad():
        # #     sample_input, sample_label = next(iter(val_loader))
        # #     sample_input = sample_input.to(device)
        # #     output_test = loaded_model(sample_input)
        # #     print(f"ë¡œë“œëœ ëª¨ë¸ ì¶œë ¥ í˜•íƒœ: {output_test.shape}")

    else:
        print(
            "ë¼ë²¨ ë˜ëŠ” ë°ì´í„° ê²½ë¡œ ë¡œë“œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë‹¤ìŒ ë‹¨ê³„ë¡œ ì§„í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        )
