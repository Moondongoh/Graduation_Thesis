import os
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, OneHotEncoder

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader

# import librosa # WAV íŒŒì¼ ë¡œë“œì—ëŠ” ì‚¬ìš©ë˜ì§€ ì•Šì§€ë§Œ, ë”ë¯¸ WAV ìƒì„± ì‹œ í•„ìš”í•  ìˆ˜ ìˆìŒ
# import pywt # ì›¨ì´ë¸”ë¦¿ ë³€í™˜ì—ëŠ” ì‚¬ìš©ë˜ì§€ ì•Šì§€ë§Œ, ë”ë¯¸ WAV ìƒì„± ì‹œ ì‚¬ìš©ë  ìˆ˜ ìˆìŒ
import soundfile as sf  # ë”ë¯¸ íŒŒì¼ ìƒì„±ì„ ìœ„í•¨

# GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
# CUDA (NVIDIA GPU) ì‚¬ìš© ê°€ëŠ¥í•˜ë©´ 'cuda', ì•„ë‹ˆë©´ 'cpu'
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")


# --- ìˆ˜ì •ëœ load_labels_and_data_paths í•¨ìˆ˜ ---
def load_labels_and_data_paths(data_base_path, label_file_path):
    """
    ë¼ë²¨ íŒŒì¼ê³¼ 1D TXT ë°ì´í„° ê²½ë¡œë¥¼ ë¡œë“œí•˜ê³  ë§¤ì¹­í•©ë‹ˆë‹¤.

    Args:
        data_base_path (str): 1D TXT íŒŒì¼ë“¤ì´ ìˆëŠ” ë””ë ‰í† ë¦¬ ê²½ë¡œ (ì˜ˆ: 'Read_1D_Data/result' ë˜ëŠ” 'Wavelet_Transformed_Data').
        label_file_path (str): ë¼ë²¨ ì •ë³´ê°€ ë‹´ê¸´ CSV ë˜ëŠ” TXT íŒŒì¼ ê²½ë¡œ (ì˜ˆ: 'REFERENCE.csv').

    Returns:
        tuple: (list of TXT file paths, list of corresponding labels)
               ë˜ëŠ” (None, None) ì˜¤ë¥˜ ë°œìƒ ì‹œ.
    """
    data_file_paths = []
    labels = []
    skipped_files_count = 0

    if not os.path.exists(label_file_path):
        print(
            f"ì˜¤ë¥˜: ë¼ë²¨ íŒŒì¼ '{label_file_path}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”."
        )
        return None, None

    try:
        df_labels = pd.read_csv(
            label_file_path, header=None, names=["filename_prefix", "label"]
        )
        print(f"'{label_file_path}'ì—ì„œ {len(df_labels)}ê°œì˜ ë¼ë²¨ ì •ë³´ë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")

        for index, row in df_labels.iterrows():
            # ì›¨ì´ë¸”ë¦¿ ì ìš© ì „ 1D ë°ì´í„°ëŠ” '_data.txt' í™•ì¥ìë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
            txt_filename = f"{row['filename_prefix']}_data.txt"
            txt_file_path = os.path.join(data_base_path, txt_filename)

            if os.path.exists(txt_file_path):
                data_file_paths.append(txt_file_path)
                labels.append(row["label"])
            else:
                # print(f"ê²½ê³ : ë§¤ì¹­ë˜ëŠ” TXT íŒŒì¼ '{txt_file_path}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.")
                skipped_files_count += 1

    except pd.errors.EmptyDataError:
        print(f"ì˜¤ë¥˜: ë¼ë²¨ íŒŒì¼ '{label_file_path}'ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
        return None, None
    except Exception as e:
        print(f"ë¼ë²¨ íŒŒì¼ ë¡œë“œ ë˜ëŠ” ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None, None

    if skipped_files_count > 0:
        print(
            f"ì´ {skipped_files_count}ê°œì˜ TXT íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ê±´ë„ˆë›°ì—ˆìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ ë‹¤ì‹œ í™•ì¸í•´ì£¼ì„¸ìš”."
        )
    return data_file_paths, labels


# --- preprocess_labels í•¨ìˆ˜ (ì´ì „ê³¼ ë™ì¼) ---
def preprocess_labels(labels):
    """
    ë¼ë²¨ (ë¬¸ìì—´ ë˜ëŠ” ìˆ«ì)ì„ ìˆ˜ì¹˜í˜• (ì¸ì½”ë”©) ë° ì›-í•« ì¸ì½”ë”© í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    "1ì´ ì •ìƒ, 0ì´ ë¹„ì •ìƒ" ê·œì¹™ì„ ë”°ë¦…ë‹ˆë‹¤.
    """
    label_encoder = LabelEncoder()
    encoded_labels = label_encoder.fit_transform(labels)

    mapped_class_names = [
        f"Class {cls_val} ({'ë¹„ì •ìƒ' if cls_val == 0 else 'ì •ìƒ'})"
        for cls_val in label_encoder.classes_
    ]

    print(f"ì›ë³¸ ë¼ë²¨ ì¢…ë¥˜: {np.unique(labels)}")
    print(f"ë‚´ë¶€ ì¸ì½”ë”©ëœ ë¼ë²¨ ì¢…ë¥˜: {np.unique(encoded_labels)}")
    print(f"ë§¤í•‘ëœ í´ë˜ìŠ¤ ì´ë¦„ (0:ë¹„ì •ìƒ, 1:ì •ìƒ): {mapped_class_names}")

    onehot_encoder = OneHotEncoder(sparse_output=False)
    onehot_labels = onehot_encoder.fit_transform(encoded_labels.reshape(-1, 1))
    print(f"ì›-í•« ì¸ì½”ë”©ëœ ë¼ë²¨ í˜•íƒœ: {onehot_labels.shape}")

    return encoded_labels, onehot_labels, label_encoder, mapped_class_names


# --- PyTorch Custom Dataset ì •ì˜ ---
class OneDTextDataset(Dataset):
    def __init__(
        self, data_file_paths, labels, max_sequence_length
    ):  # labelsëŠ” ì´ì œ ì •ìˆ˜í˜• (encoded_labels)
        self.data_file_paths = data_file_paths
        # ë¼ë²¨ì„ float32 ëŒ€ì‹  long íƒ€ì…ìœ¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
        # PyTorchì˜ CrossEntropyLossëŠ” íƒ€ê²Ÿ ë¼ë²¨ì„ long íƒ€ì…ìœ¼ë¡œ ê¸°ëŒ€í•©ë‹ˆë‹¤.
        self.labels = torch.tensor(labels, dtype=torch.long)
        self.max_sequence_length = max_sequence_length
        self.cached_data = {}  # ë°ì´í„°ë¥¼ ë¯¸ë¦¬ ë¡œë“œí•˜ì—¬ ìºì‹±

        print(f"ë°ì´í„°ì…‹ ì´ˆê¸°í™” ì¤‘. ì´ {len(data_file_paths)}ê°œ íŒŒì¼ ì²˜ë¦¬ ì˜ˆìƒ...")
        for i, path in enumerate(self.data_file_paths):
            try:
                y_data = np.loadtxt(path, dtype=np.float32)

                if len(y_data) < self.max_sequence_length:
                    pad_width = self.max_sequence_length - len(y_data)
                    y_padded = np.pad(
                        y_data, (0, pad_width), "constant", constant_values=0
                    )
                else:
                    y_padded = y_data[: self.max_sequence_length]

                # PyTorch Conv1DëŠ” (batch_size, channels, sequence_length) í˜•íƒœë¥¼ ê¸°ëŒ€
                # ì—¬ê¸°ì„œëŠ” channels=1 ì´ë¯€ë¡œ (1, sequence_length) í˜•íƒœë¡œ ì €ì¥
                self.cached_data[i] = torch.tensor(
                    y_padded.reshape(1, -1), dtype=torch.float32
                )

            except Exception as e:
                print(
                    f"  ë°ì´í„°ì…‹ ë¡œë“œ ì˜¤ë¥˜: '{os.path.basename(path)}' - {e}. í•´ë‹¹ ì¸ë±ìŠ¤ ê±´ë„ˆëœœ."
                )
                self.cached_data[i] = None  # ì˜¤ë¥˜ ë°œìƒ íŒŒì¼ì€ Noneìœ¼ë¡œ í‘œì‹œ

        # ì˜¤ë¥˜ ì—†ì´ ë¡œë“œëœ ë°ì´í„°ë§Œ ì‚¬ìš©í•˜ë„ë¡ í•„í„°ë§
        # self.valid_indicesë¥¼ ì‚¬ìš©í•˜ì—¬ __len__ê³¼ __getitem__ì—ì„œ ì°¸ì¡°
        self.valid_indices = [
            i for i, data in self.cached_data.items() if data is not None
        ]
        print(f"ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ. ìœ íš¨í•œ íŒŒì¼ ìˆ˜: {len(self.valid_indices)}")

    def __len__(self):
        return len(self.valid_indices)

    def __getitem__(self, idx):
        # ì‹¤ì œ ì¸ë±ìŠ¤ ë§¤í•‘ (self.valid_indicesì— ë”°ë¼)
        original_idx = self.valid_indices[idx]
        data = self.cached_data[original_idx]
        label = self.labels[original_idx]  # self.labels_onehot ëŒ€ì‹  self.labels ì‚¬ìš©
        return data, label


# --- PyTorch 1D CNN ëª¨ë¸ ì •ì˜ ---
class OneDCNN(nn.Module):
    def __init__(
        self, num_classes, input_length
    ):  # input_lengthë¥¼ íŒŒë¼ë¯¸í„°ë¡œ ë°›ë„ë¡ ë³€ê²½
        super(OneDCNN, self).__init__()
        self.conv_layers = nn.Sequential(
            # Conv1dëŠ” (in_channels, out_channels, kernel_size)
            # input_shapeì€ (batch_size, channels, sequence_length)ê°€ ë¨
            nn.Conv1d(in_channels=1, out_channels=32, kernel_size=3),
            nn.ReLU(),
            nn.MaxPool1d(kernel_size=2),
            nn.Dropout(0.3),
            nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3),
            nn.ReLU(),
            nn.MaxPool1d(kernel_size=2),
            nn.Dropout(0.3),
        )

        # Flattening ì „ feature map í¬ê¸° ê³„ì‚°
        self._calculate_flatten_size(input_length)

        self.fc_layers = nn.Sequential(
            nn.Linear(self._flatten_size, 100),  # _flatten_sizeëŠ” ê³„ì‚°ëœ ê°’
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(100, num_classes),
            # CrossEntropyLossëŠ” ë‚´ë¶€ì ìœ¼ë¡œ Softmaxë¥¼ í¬í•¨í•˜ë¯€ë¡œ ìµœì¢… SoftmaxëŠ” í•„ìš” ì—†ìŒ
        )

    def _calculate_flatten_size(self, input_length):
        # ë”ë¯¸ í…ì„œë¥¼ ë§Œë“¤ì–´ì„œ Conv ì¸µì„ í†µê³¼ì‹œì¼œ ì¶œë ¥ í¬ê¸°ë¥¼ ê³„ì‚°
        with torch.no_grad():
            dummy_input = torch.zeros(1, 1, input_length)  # (batch, channels, length)
            dummy_output = self.conv_layers(dummy_input)
            self._flatten_size = (
                dummy_output.numel() // dummy_output.shape[0]
            )  # ë°°ì¹˜ ì°¨ì› ì œì™¸

    def forward(self, x):
        x = self.conv_layers(x)
        x = x.view(x.size(0), -1)  # Flatten
        x = self.fc_layers(x)
        return x


# --- ì‚¬ìš© ì˜ˆì‹œ (ë©”ì¸ ì½”ë“œ) ---
if __name__ == "__main__":
    # ğŸš¨ğŸš¨ğŸš¨ ì‹¤ì œ ë°ì´í„°ì…‹ ê²½ë¡œì™€ REFERENCE.csv ê²½ë¡œë¡œ ë³€ê²½í•´ì•¼ í•©ë‹ˆë‹¤ ğŸš¨ğŸš¨ğŸš¨
    # ==========================================================ê²½ë¡œ ì§€ì •==========================================================
    original_1d_txt_data_base_path = r"D:\MDO\heartbeat\Wavelet\Normal_Transformed_Data"
    label_file_path = r"D:\MDO\heartbeat\Wavelet/Normal_Transformed_Data/REFERENCE.csv"

    # â—â—â— í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ ë”ë¯¸ íŒŒì¼ ë° í´ë” ìƒì„± (ì‹¤ì œ ì‚¬ìš© ì‹œì—ëŠ” ì´ ë¶€ë¶„ ì‚­ì œ) â—â—â—
    if not os.path.exists(original_1d_txt_data_base_path) or not os.path.exists(
        label_file_path
    ):
        print("\n--- í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ë”ë¯¸ ì›ë³¸ 1D TXT ë°ì´í„° í´ë” ë° íŒŒì¼ ìƒì„± ---")
        os.makedirs(original_1d_txt_data_base_path, exist_ok=True)

        dummy_labels_data = [
            ("a0001", 1),
            ("a0002", 0),
            ("b0001", 1),
            ("c0001", 1),
            ("c0002", 0),
            ("a0003", 1),
            ("b0002", 0),
            ("c0003", 1),
            ("a0004", 0),
            ("b0003", 1),
            ("d0001", 1),
            ("d0002", 0),
            ("e0001", 1),
            ("f0001", 0),
        ]
        dummy_df = pd.DataFrame(dummy_labels_data, columns=["filename_prefix", "label"])
        dummy_df.to_csv(label_file_path, index=False, header=False)
        print(f"ë”ë¯¸ '{label_file_path}' ìƒì„± ì™„ë£Œ (ë¼ë²¨: 0=ë¹„ì •ìƒ, 1=ì •ìƒ).")

        dummy_sr = 44100
        for prefix, _ in dummy_labels_data:
            dummy_txt_path = os.path.join(
                original_1d_txt_data_base_path, f"{prefix}_data.txt"
            )
            dummy_duration = np.random.randint(2, 6)
            dummy_data = np.random.uniform(
                -0.5, 0.5, int(dummy_sr * dummy_duration)
            ).astype(np.float32)
            np.savetxt(dummy_txt_path, dummy_data, fmt="%f", delimiter="\n")
        print("--- ë”ë¯¸ ì›ë³¸ 1D TXT íŒŒì¼ ìƒì„± ì™„ë£Œ. ì´ì œ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ---")
    # â—â—â— ë”ë¯¸ íŒŒì¼ ìƒì„± ë â—â—â—

    # 1. ë¼ë²¨ ë° 1D TXT ë°ì´í„° ê²½ë¡œ ë¡œë“œ
    data_file_paths, raw_labels = load_labels_and_data_paths(
        original_1d_txt_data_base_path, label_file_path
    )

    if data_file_paths and raw_labels:
        print("\n--- ë¼ë²¨ ì „ì²˜ë¦¬ ---")
        # encoded_labels (ì •ìˆ˜í˜•)ì™€ onehot_labels (ì›-í•«) ëª¨ë‘ ì–»ìŠµë‹ˆë‹¤.
        # encoded_labelsëŠ” CrossEntropyLossë¥¼ ìœ„í•´ ì‚¬ìš©í•˜ê³ , onehot_labelsëŠ” í´ë˜ìŠ¤ ìˆ˜ í™•ì¸ì— ì‚¬ìš©í•©ë‹ˆë‹¤.
        encoded_labels, onehot_labels, label_encoder_obj, class_names_mapping = (
            preprocess_labels(raw_labels)
        )

        print(f"\në¡œë“œëœ ìœ íš¨ 1D TXT íŒŒì¼ ìˆ˜: {len(data_file_paths)}")
        print(f"ë¡œë“œëœ ìœ íš¨ ë¼ë²¨ ìˆ˜: {len(onehot_labels)}")

        # 2. í›ˆë ¨ ì„¸íŠ¸ì™€ ê²€ì¦ ì„¸íŠ¸ë¡œ ë¶„ë¦¬
        # PyTorch Datasetì— ì „ë‹¬í•˜ê¸° ìœ„í•´ ì •ìˆ˜í˜• ë¼ë²¨(encoded_labels)ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
        X_train_paths, X_val_paths, y_train_encoded, y_val_encoded = train_test_split(
            data_file_paths,
            encoded_labels,
            test_size=0.2,
            random_state=42,
            stratify=encoded_labels,
        )

        print(f"\ní›ˆë ¨ ì„¸íŠ¸ í¬ê¸° (ê²½ë¡œ): {len(X_train_paths)}")
        print(f"ê²€ì¦ ì„¸íŠ¸ í¬ê¸° (ê²½ë¡œ): {len(X_val_paths)}")

        # 3. PyTorch Dataset ë° DataLoader ì¤€ë¹„
        DEFAULT_SAMPLE_RATE = 44100
        MAX_ORIGINAL_TXT_SEQUENCE_LENGTH = (
            DEFAULT_SAMPLE_RATE * 5
        )  # 5ì´ˆ ì˜¤ë””ì˜¤ë¥¼ ê¸°ì¤€ìœ¼ë¡œ í•¨ (220500 ìƒ˜í”Œ)

        print(f"\n--- í›ˆë ¨ ë°ì´í„°ì…‹ ìƒì„± (1D TXT ë°ì´í„° ì‚¬ìš©) ---")
        # OneDTextDatasetì— ì •ìˆ˜í˜• ë¼ë²¨(y_train_encoded)ì„ ì „ë‹¬
        train_dataset = OneDTextDataset(
            X_train_paths, y_train_encoded, MAX_ORIGINAL_TXT_SEQUENCE_LENGTH
        )
        train_loader = DataLoader(
            train_dataset, batch_size=32, shuffle=True, num_workers=0
        )  # num_workersëŠ” í™˜ê²½ì— ë”°ë¼ ì¡°ì • (Windowsì—ì„œëŠ” 0 ê¶Œì¥)
        print(f"í›ˆë ¨ ë°ì´í„°ì…‹ í¬ê¸°: {len(train_dataset)}")

        print(f"\n--- ê²€ì¦ ë°ì´í„°ì…‹ ìƒì„± (1D TXT ë°ì´í„° ì‚¬ìš©) ---")
        # OneDTextDatasetì— ì •ìˆ˜í˜• ë¼ë²¨(y_val_encoded)ì„ ì „ë‹¬
        val_dataset = OneDTextDataset(
            X_val_paths, y_val_encoded, MAX_ORIGINAL_TXT_SEQUENCE_LENGTH
        )
        val_loader = DataLoader(
            val_dataset, batch_size=32, shuffle=False, num_workers=0
        )
        print(f"ê²€ì¦ ë°ì´í„°ì…‹ í¬ê¸°: {len(val_dataset)}")

        # 4. PyTorch 1D CNN ëª¨ë¸ êµ¬ì¶• ë° í•™ìŠµ
        num_classes = onehot_labels.shape[
            1
        ]  # num_classesëŠ” ì›-í•« ì¸ì½”ë”©ëœ ë¼ë²¨ì˜ ì»¬ëŸ¼ ìˆ˜
        # ëª¨ë¸ ìƒì„± ì‹œ MAX_ORIGINAL_TXT_SEQUENCE_LENGTHë¥¼ ì „ë‹¬í•˜ì—¬ Flatten í¬ê¸° ê³„ì‚°ì— ì‚¬ìš©
        model = OneDCNN(
            num_classes=num_classes, input_length=MAX_ORIGINAL_TXT_SEQUENCE_LENGTH
        ).to(device)

        # ì˜µí‹°ë§ˆì´ì €ì™€ ì†ì‹¤ í•¨ìˆ˜ ì •ì˜
        optimizer = optim.Adam(model.parameters(), lr=0.001)
        # CrossEntropyLossëŠ” íƒ€ê²Ÿì´ í´ë˜ìŠ¤ ì¸ë±ìŠ¤ (ì •ìˆ˜, Long íƒ€ì…)ì—¬ì•¼ í•˜ë©°, ë‚´ë¶€ì ìœ¼ë¡œ Softmax í¬í•¨
        criterion = nn.CrossEntropyLoss()

        print("\n--- PyTorch 1D CNN ëª¨ë¸ í•™ìŠµ ì‹œì‘ (1D TXT ë°ì´í„°) ---")
        num_epochs = 30

        for epoch in range(num_epochs):
            model.train()  # í›ˆë ¨ ëª¨ë“œ
            running_loss = 0.0
            correct_train = 0
            total_train = 0

            for i, (inputs, labels) in enumerate(train_loader):
                inputs, labels = inputs.to(device), labels.to(device)

                optimizer.zero_grad()  # ê¸°ìš¸ê¸° ì´ˆê¸°í™”
                outputs = model(inputs)
                loss = criterion(outputs, labels)  # outputsëŠ” float, labelsëŠ” long
                loss.backward()  # ì—­ì „íŒŒ
                optimizer.step()  # ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸

                running_loss += loss.item() * inputs.size(0)
                _, predicted = torch.max(
                    outputs.data, 1
                )  # ì˜ˆì¸¡ì€ outputsì—ì„œ ê°€ì¥ ë†’ì€ í™•ë¥ ì˜ ì¸ë±ìŠ¤
                total_train += labels.size(0)
                correct_train += (predicted == labels).sum().item()

            epoch_loss = running_loss / len(train_dataset)
            epoch_acc = 100 * correct_train / total_train

            # ê²€ì¦ ë‹¨ê³„
            model.eval()  # í‰ê°€ ëª¨ë“œ
            val_loss = 0.0
            correct_val = 0
            total_val = 0
            with torch.no_grad():  # ê¸°ìš¸ê¸° ê³„ì‚° ë¹„í™œì„±í™”
                for inputs_val, labels_val in val_loader:
                    inputs_val, labels_val = inputs_val.to(device), labels_val.to(
                        device
                    )
                    outputs_val = model(inputs_val)
                    loss_val = criterion(outputs_val, labels_val)

                    val_loss += loss_val.item() * inputs_val.size(0)
                    _, predicted_val = torch.max(outputs_val.data, 1)
                    total_val += labels_val.size(0)
                    correct_val += (predicted_val == labels_val).sum().item()

            val_epoch_loss = val_loss / len(val_dataset)
            val_epoch_acc = 100 * correct_val / total_val

            print(
                f"Epoch [{epoch+1}/{num_epochs}], "
                f"Train Loss: {epoch_loss:.4f}, Train Acc: {epoch_acc:.2f}%, "
                f"Val Loss: {val_epoch_loss:.4f}, Val Acc: {val_epoch_acc:.2f}%"
            )

        print("PyTorch 1D CNN ëª¨ë¸ í•™ìŠµ ì™„ë£Œ (1D TXT ë°ì´í„°).")

        # --- ëª¨ë¸ ì €ì¥ ---
        # ==========================================================ê²½ë¡œ ì§€ì •==========================================================
        model_save_dir = "./Wavelt/model"
        os.makedirs(model_save_dir, exist_ok=True)
        model_save_path_original_txt = os.path.join(
            model_save_dir, "original_1d_txt_cnn_model.pth"
        )

        # ëª¨ë¸ì˜ ìƒíƒœ ì‚¬ì „(state_dict) ì €ì¥
        torch.save(model.state_dict(), model_save_path_original_txt)
        print(
            f"\nëª¨ë¸ì´ ë‹¤ìŒ ê²½ë¡œì— ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: '{os.path.abspath(model_save_path_original_txt)}'"
        )

    else:
        print(
            "ë¼ë²¨ ë˜ëŠ” ë°ì´í„° ê²½ë¡œ ë¡œë“œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë‹¤ìŒ ë‹¨ê³„ë¡œ ì§„í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        )
