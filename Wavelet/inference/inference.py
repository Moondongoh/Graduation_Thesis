import os
import pandas as pd
import numpy as np
import tensorflow as tf
from sklearn.preprocessing import (
    LabelEncoder,
    OneHotEncoder,
)  # ë¼ë²¨ ì „ì²˜ë¦¬ë¥¼ ìœ„í•¨ (í‰ê°€ ì‹œ í•„ìš”)


# --- load_labels_and_data_paths í•¨ìˆ˜ (ì´ì „ê³¼ ë™ì¼) ---
def load_labels_and_data_paths(data_base_path, label_file_path):
    """
    ë¼ë²¨ íŒŒì¼ê³¼ 1D TXT ë°ì´í„° ê²½ë¡œë¥¼ ë¡œë“œí•˜ê³  ë§¤ì¹­í•©ë‹ˆë‹¤.
    Args:
        data_base_path (str): 1D TXT íŒŒì¼ë“¤ì´ ìˆëŠ” ë””ë ‰í† ë¦¬ ê²½ë¡œ.
        label_file_path (str): ë¼ë²¨ ì •ë³´ê°€ ë‹´ê¸´ CSV ë˜ëŠ” TXT íŒŒì¼ ê²½ë¡œ.
    Returns:
        tuple: (list of TXT file paths, list of corresponding labels)
               ë˜ëŠ” (None, None) ì˜¤ë¥˜ ë°œìƒ ì‹œ.
    """
    data_file_paths = []
    labels = []
    skipped_files_count = 0

    if not os.path.exists(label_file_path):
        print(
            f"ì˜¤ë¥˜: ë¼ë²¨ íŒŒì¼ '{label_file_path}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”."
        )
        return None, None

    try:
        df_labels = pd.read_csv(
            label_file_path, header=None, names=["filename_prefix", "label"]
        )
        print(f"'{label_file_path}'ì—ì„œ {len(df_labels)}ê°œì˜ ë¼ë²¨ ì •ë³´ë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")

        for index, row in df_labels.iterrows():
            # ì›ë³¸ 1D ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ '_data.txt' í™•ì¥ìë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
            txt_filename = f"{row['filename_prefix']}_data.txt"
            txt_file_path = os.path.join(data_base_path, txt_filename)

            if os.path.exists(txt_file_path):
                data_file_paths.append(txt_file_path)
                labels.append(row["label"])
            else:
                print(
                    f"ê²½ê³ : ë§¤ì¹­ë˜ëŠ” TXT íŒŒì¼ '{txt_file_path}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤."
                )
                skipped_files_count += 1

    except pd.errors.EmptyDataError:
        print(f"ì˜¤ë¥˜: ë¼ë²¨ íŒŒì¼ '{label_file_path}'ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
        return None, None
    except Exception as e:
        print(f"ë¼ë²¨ íŒŒì¼ ë¡œë“œ ë˜ëŠ” ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None, None

    if skipped_files_count > 0:
        print(
            f"ì´ {skipped_files_count}ê°œì˜ TXT íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ê±´ë„ˆë›°ì—ˆìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ ë‹¤ì‹œ í™•ì¸í•´ì£¼ì„¸ìš”."
        )
    return data_file_paths, labels


# --- preprocess_labels í•¨ìˆ˜ (ì´ì „ê³¼ ë™ì¼) ---
def preprocess_labels(labels):
    """
    ë¼ë²¨ (ë¬¸ìì—´ ë˜ëŠ” ìˆ«ì)ì„ ìˆ˜ì¹˜í˜• (ì¸ì½”ë”©) ë° ì›-í•« ì¸ì½”ë”© í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    "1ì´ ì •ìƒ, 0ì´ ë¹„ì •ìƒ" ê·œì¹™ì„ ë”°ë¦…ë‹ˆë‹¤.
    """
    label_encoder = LabelEncoder()
    encoded_labels = label_encoder.fit_transform(labels)

    mapped_class_names = [
        f"Class {cls_val} ({'ë¹„ì •ìƒ' if cls_val == 0 else 'ì •ìƒ'})"
        for cls_val in label_encoder.classes_
    ]

    print(f"ì›ë³¸ ë¼ë²¨ ì¢…ë¥˜: {np.unique(labels)}")
    print(f"ë‚´ë¶€ ì¸ì½”ë”©ëœ ë¼ë²¨ ì¢…ë¥˜: {np.unique(encoded_labels)}")
    print(f"ë§¤í•‘ëœ í´ë˜ìŠ¤ ì´ë¦„ (0:ë¹„ì •ìƒ, 1:ì •ìƒ): {mapped_class_names}")

    onehot_encoder = OneHotEncoder(sparse_output=False)
    onehot_labels = onehot_encoder.fit_transform(encoded_labels.reshape(-1, 1))
    print(f"ì›-í•« ì¸ì½”ë”©ëœ ë¼ë²¨ í˜•íƒœ: {onehot_labels.shape}")

    return encoded_labels, onehot_labels, label_encoder, mapped_class_names


# --- load_data_for_cnn í•¨ìˆ˜ (ì´ì „ê³¼ ë™ì¼) ---
def load_data_for_cnn(data_file_paths, max_sequence_length):
    """
    ì£¼ì–´ì§„ TXT íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸ì—ì„œ ìˆ«ì ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  CNN ì…ë ¥ì„ ìœ„í•´ ì „ì²˜ë¦¬í•©ë‹ˆë‹¤.
    ëª¨ë“  ë°ì´í„° ì‹œí€€ìŠ¤ì˜ ê¸¸ì´ë¥¼ max_sequence_lengthë¡œ í†µì¼í•©ë‹ˆë‹¤ (íŒ¨ë”© ë˜ëŠ” ìë¥´ê¸°).
    """
    processed_data = []

    for i, path in enumerate(data_file_paths):
        try:
            y_data = np.loadtxt(path)

            if len(y_data) < max_sequence_length:
                pad_width = max_sequence_length - len(y_data)
                y_padded = np.pad(y_data, (0, pad_width), "constant")
            else:
                y_padded = y_data[:max_sequence_length]

            processed_data.append(y_padded.reshape(-1, 1))

            if i % 100 == 0:
                print(
                    f"  {i+1}/{len(data_file_paths)} íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ: {os.path.basename(path)}"
                )

        except Exception as e:
            print(
                f"  ì˜¤ë¥˜: '{os.path.basename(path)}' ì²˜ë¦¬ ì¤‘ ë¬¸ì œ ë°œìƒ - {e}. í•´ë‹¹ íŒŒì¼ ê±´ë„ˆëœ€."
            )

    return np.array(processed_data)


if __name__ == "__main__":
    # ğŸš¨ğŸš¨ğŸš¨ ì‹¤ì œ ëª¨ë¸ ê²½ë¡œ ë° ê²€ì¦ ë°ì´í„° ê²½ë¡œë¥¼ í™˜ê²½ì— ë§ê²Œ ì„¤ì •í•´ì£¼ì„¸ìš” ğŸš¨ğŸš¨ğŸš¨

    # 1. í•™ìŠµëœ ëª¨ë¸ íŒŒì¼ ê²½ë¡œ
    # ëª¨ë¸ í•™ìŠµ ì‹œ ì €ì¥ëœ ì •í™•í•œ .keras íŒŒì¼ ê²½ë¡œë¥¼ ì§€ì •í•´ì•¼ í•©ë‹ˆë‹¤.
    # ì˜ˆ: 'D:/MDO/heartbeat/Wavelet/saved_models/wavelet_cnn_model.keras'
    # ==========================================================ê²½ë¡œ ì§€ì •==========================================================
    model_path = r".\Wavelet\model\wavelet_cnn_model.keras"

    # 2. ì¶”ë¡ í•  ì›ë³¸ 1D ë°ì´í„° í´ë” ê²½ë¡œ
    # './Wavelet/validation/Validation_Tansformed_Data' ì— ì›ë³¸ 1D TXT íŒŒì¼ë“¤ì´ ìˆë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.
    # ==========================================================ê²½ë¡œ ì§€ì •==========================================================
    validation_data_folder = r"D:\MDO\heartbeat\Wavelet\Validation_Transformed_Data"

    # 3. ê²€ì¦ ë°ì´í„°ì— ëŒ€í•œ ë¼ë²¨ íŒŒì¼ ê²½ë¡œ (í‰ê°€ ì‹œ í•„ìš”)
    # ì¼ë°˜ì ìœ¼ë¡œ REFERENCE.csvëŠ” ë°ì´í„°ì…‹ì˜ ìµœìƒìœ„ í´ë”ì— ìˆìŠµë‹ˆë‹¤.
    # ë§Œì•½ ê²€ì¦ ë°ì´í„°ì…‹ë§Œì„ ìœ„í•œ ë³„ë„ì˜ REFERENCE.csvê°€ ìˆë‹¤ë©´ í•´ë‹¹ ê²½ë¡œë¥¼ ì§€ì •í•˜ì„¸ìš”.
    # ==========================================================ê²½ë¡œ ì§€ì •==========================================================
    validation_label_file_path = (
        r"D:\MDO\heartbeat\Wavelet/Validation_Transformed_Data\REFERENCE.csv"
    )

    # 4. í•™ìŠµ ì‹œ ì‚¬ìš©í–ˆë˜ MAX_SEQUENCE_LENGTH ê°’ (ê°€ì¥ ì¤‘ìš”!)
    # ğŸš¨ğŸš¨ğŸš¨ ì´ ê°’ì„ ëª¨ë¸ í•™ìŠµ ì‹œ ì‚¬ìš©í•œ ì •í™•í•œ ê°’ìœ¼ë¡œ ë‹¤ì‹œ ë³€ê²½í•©ë‹ˆë‹¤! ğŸš¨ğŸš¨ğŸš¨
    # í˜„ì¬ ë¡œë“œëœ ëª¨ë¸ì€ 2000 ê¸¸ì´ì˜ ì‹œí€€ìŠ¤ë¡œ í•™ìŠµë˜ì—ˆìŠµë‹ˆë‹¤.
    MAX_SEQUENCE_LENGTH_WAVELET = 2000  # ğŸš¨ğŸš¨ğŸš¨ ë‹¤ì‹œ 2000ìœ¼ë¡œ ìˆ˜ì •ë¨! ğŸš¨ğŸš¨ğŸš¨

    print(f"\n--- ëª¨ë¸ ë¡œë“œ ì‹œì‘: {model_path} ---")
    try:
        loaded_model = tf.keras.models.load_model(model_path)
        print(f"ëª¨ë¸ì´ '{model_path}'ì—ì„œ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
        loaded_model.summary()  # ëª¨ë¸ ë¡œë“œ í›„ summaryë¥¼ ë‹¤ì‹œ ì¶œë ¥í•˜ì—¬ input shape í™•ì¸
    except Exception as e:
        print(f"ì˜¤ë¥˜: ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ - {e}")
        print("ëª¨ë¸ ê²½ë¡œë¥¼ ë‹¤ì‹œ í™•ì¸í•˜ê±°ë‚˜, ëª¨ë¸ì´ ì†ìƒë˜ì§€ ì•Šì•˜ëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
        exit()  # ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ ì‹œ ìŠ¤í¬ë¦½íŠ¸ ì¢…ë£Œ

    print(f"\n--- ê²€ì¦ ë°ì´í„° ë¡œë“œ ì‹œì‘: {validation_data_folder} ---")
    # load_labels_and_data_paths í•¨ìˆ˜ëŠ” ì´ì œ _data.txt íŒŒì¼ì„ ì°¾ìŠµë‹ˆë‹¤.
    val_data_file_paths, val_raw_labels = load_labels_and_data_paths(
        validation_data_folder, validation_label_file_path
    )

    if val_data_file_paths and val_raw_labels:
        print("\n--- ê²€ì¦ ë°ì´í„° ë¼ë²¨ ì „ì²˜ë¦¬ ---")
        (
            val_encoded_labels,
            val_onehot_labels,
            val_label_encoder_obj,
            val_class_names_mapping,
        ) = preprocess_labels(val_raw_labels)

        print(f"\në¡œë“œëœ ìœ íš¨ ê²€ì¦ ì›ë³¸ 1D TXT íŒŒì¼ ìˆ˜: {len(val_data_file_paths)}")
        print(f"ë¡œë“œëœ ìœ íš¨ ê²€ì¦ ë¼ë²¨ ìˆ˜: {len(val_onehot_labels)}")

        print(f"\n--- ê²€ì¦ ë°ì´í„° ì „ì²˜ë¦¬ (CNN ì…ë ¥ í˜•íƒœ) ---")
        X_val_processed = load_data_for_cnn(
            val_data_file_paths, MAX_SEQUENCE_LENGTH_WAVELET
        )
        print(f"ì „ì²˜ë¦¬ëœ ê²€ì¦ ë°ì´í„° í˜•íƒœ: {X_val_processed.shape}")

        if X_val_processed.shape[0] == 0:
            print(
                "ì²˜ë¦¬ëœ ê²€ì¦ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ ê²½ë¡œ ë° ì´ë¦„ íŒ¨í„´ì„ ë‹¤ì‹œ í™•ì¸í•´ì£¼ì„¸ìš”."
            )
        else:
            print("\n--- ì¶”ë¡  ìˆ˜í–‰ ---")
            predictions = loaded_model.predict(X_val_processed)
            predicted_classes = np.argmax(
                predictions, axis=1
            )  # ê°€ì¥ ë†’ì€ í™•ë¥ ì„ ê°€ì§„ í´ë˜ìŠ¤ ì¸ë±ìŠ¤

            # ì˜ˆì¸¡ ê²°ê³¼ ë° ì‹¤ì œ ë¼ë²¨ ë§¤í•‘ (0:ë¹„ì •ìƒ, 1:ì •ìƒ)
            print("\n--- ì¶”ë¡  ê²°ê³¼ ---")
            for i, pred_class_idx in enumerate(predicted_classes):
                true_label = val_raw_labels[i]
                predicted_label = val_label_encoder_obj.inverse_transform(
                    [pred_class_idx]
                )[0]

                # ë¼ë²¨ì´ '0'ì´ë©´ 'ë¹„ì •ìƒ', '1'ì´ë©´ 'ì •ìƒ'ìœ¼ë¡œ ë§¤í•‘í•˜ì—¬ ì¶œë ¥
                true_label_str = "ì •ìƒ" if true_label == 1 else "ë¹„ì •ìƒ"
                predicted_label_str = "ì •ìƒ" if predicted_label == 1 else "ë¹„ì •ìƒ"

                print(
                    f"íŒŒì¼: {os.path.basename(val_data_file_paths[i])} | ì‹¤ì œ ë¼ë²¨: {true_label_str} | ì˜ˆì¸¡ ë¼ë²¨: {predicted_label_str} (í™•ë¥ : {predictions[i]})"
                )

            # ëª¨ë¸ í‰ê°€ (ì„ íƒ ì‚¬í•­)
            print("\n--- ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ---")
            loss, accuracy = loaded_model.evaluate(
                X_val_processed, val_onehot_labels, verbose=0
            )
            print(f"ê²€ì¦ ë°ì´í„° ì†ì‹¤: {loss:.4f}")
            print(f"ê²€ì¦ ë°ì´í„° ì •í™•ë„: {accuracy:.4f}")

    else:
        print("ê²€ì¦ ë°ì´í„° ë¡œë“œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì¶”ë¡ ì„ ì§„í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
